#!/usr/bin/env python3
"""
Generate comprehensive summary of sequential backtest evaluation results.

This script loads evaluation results and produces detailed statistics including:
- Model rankings by portfolio cost
- Selector performance and model distribution
- Cost breakdowns
- Per-SKU statistics

Usage:
    python scripts/generate_evaluation_summary.py

Requires:
    - Sequential results in models/results/sequential_results_*.parquet
    - Model totals in models/results/model_totals_*.parquet
    - Selector map in models/results/selector_map_*.parquet

Output:
    - Printed summary to console
    - Can be redirected to file for documentation
"""

import pandas as pd
import numpy as np
import sys
from pathlib import Path


def load_latest_results(results_dir: Path) -> tuple:
    """
    Load the most recent evaluation results.
    
    Args:
        results_dir: Directory containing results files
    
    Returns:
        (results_df, totals_df, selector_df)
    """
    # Find most recent files
    results_files = sorted(results_dir.glob('sequential_results_seq12_*.parquet'))
    totals_files = sorted(results_dir.glob('model_totals_seq12_*.parquet'))
    selector_files = sorted(results_dir.glob('selector_map_seq12_*.parquet'))
    
    if not results_files or not totals_files or not selector_files:
        raise FileNotFoundError('Results files not found in models/results/')
    
    results = pd.read_parquet(results_files[-1])
    totals = pd.read_parquet(totals_files[-1])
    selector = pd.read_parquet(selector_files[-1])
    
    return results, totals, selector


def print_header(title: str):
    """Print formatted section header."""
    print()
    print('='*80)
    print(title)
    print('='*80)
    print()


def print_model_rankings(totals: pd.DataFrame, selector_cost: float):
    """Print model rankings table."""
    print_header('Model Rankings by Portfolio Cost')
    
    print(f"{'Rank':<6} {'Model':<25} {'Portfolio Cost':>15} {'Mean SKU':>12} {'Median':>10}")
    print('-'*80)
    
    # Add selector as first row
    print(f"{'1':<6} {'SELECTOR':<25} {selector_cost:>15,.2f} {'N/A':>12} {'N/A':>10}")
    
    # Print other models
    for i, row in totals.iterrows():
        rank = i + 2  # +2 because selector is rank 1 and i is 0-indexed
        print(f"{rank:<6} {row['model_name']:<25} {row['portfolio_cost']:>15,.2f} "
              f"{row['mean_sku_cost']:>12.2f} {row['p50_sku']:>10.2f}")


def print_selector_analysis(selector: pd.DataFrame):
    """Print selector performance and model distribution."""
    print_header('Selector Analysis')
    
    print('Performance:')
    print('-'*80)
    print(f"Portfolio cost: {selector['total_cost'].sum():,.2f}")
    print(f"Mean SKU cost: {selector['total_cost'].mean():.2f}")
    print(f"Median SKU cost: {selector['total_cost'].median():.2f}")
    print(f"Std dev: {selector['total_cost'].std():.2f}")
    print()
    
    print('Model Distribution:')
    print('-'*80)
    model_counts = selector['model_name'].value_counts()
    print(f"{'Model':<25} {'Count':>8} {'Percentage':>12}")
    print('-'*80)
    for model, count in model_counts.head(10).items():
        pct = count / len(selector) * 100
        print(f"{model:<25} {count:>8} {pct:>11.1f}%")
    
    if len(model_counts) > 10:
        others = model_counts.iloc[10:].sum()
        pct = others / len(selector) * 100
        print(f"{'Others':<25} {others:>8} {pct:>11.1f}%")


def print_cost_breakdown(results: pd.DataFrame, model_name: str = 'zinb'):
    """Print cost breakdown for a specific model."""
    print_header(f'Cost Breakdown: {model_name.upper()}')
    
    model_results = results[results['model_name'] == model_name]
    
    if len(model_results) == 0:
        print(f'No results found for model: {model_name}')
        return
    
    print('Total Costs:')
    print('-'*80)
    print(f"Total cost (all weeks): {model_results['total_cost'].sum():,.2f}")
    print(f"Total cost (excl week 1): {model_results['total_cost_excl_w1'].sum():,.2f}")
    week1_cost = (model_results['total_cost'] - model_results['total_cost_excl_w1']).sum()
    print(f"Week 1 cost: {week1_cost:,.2f} "
          f"({week1_cost / model_results['total_cost'].sum() * 100:.1f}% of total)")
    print()
    
    print('Expected vs Realized:')
    print('-'*80)
    print(f"Total expected cost: {model_results['total_expected_cost'].sum():,.2f}")
    print(f"Total realized cost: {model_results['total_cost'].sum():,.2f}")
    diff = model_results['total_cost'].sum() - model_results['total_expected_cost'].sum()
    print(f"Difference: {diff:+,.2f}")


def print_data_quality(results: pd.DataFrame):
    """Print data quality metrics."""
    print_header('Data Quality')
    
    print('Missing Forecasts by Model:')
    print('-'*80)
    missing = results.groupby('model_name')['n_missing'].sum().sort_values(ascending=False)
    print(f"{'Model':<25} {'Missing Forecasts':>20}")
    print('-'*80)
    for model, count in missing.head(10).items():
        print(f"{model:<25} {int(count):>20}")


def main():
    """Main entry point."""
    print_header('SEQUENTIAL 12-WEEK BACKTEST EVALUATION SUMMARY')
    
    # Load results
    results_dir = Path('models/results')
    try:
        results, totals, selector = load_latest_results(results_dir)
    except FileNotFoundError as e:
        print(f'ERROR: {e}')
        sys.exit(1)
    
    # Basic info
    print(f"Total evaluations: {len(results):,}")
    print(f"Unique SKUs: {len(results.groupby(['store', 'product'])):,}")
    print(f"Models evaluated: {results['model_name'].nunique()}")
    
    # Model rankings
    selector_cost = selector['total_cost'].sum()
    print_model_rankings(totals, selector_cost)
    
    # Selector analysis
    print_selector_analysis(selector)
    
    # Cost breakdown
    print_cost_breakdown(results, 'zinb')
    
    # Data quality
    print_data_quality(results)
    
    # Summary
    print_header('Summary')
    best_model = totals.iloc[0]['model_name']
    best_cost = totals.iloc[0]['portfolio_cost']
    improvement = (best_cost - selector_cost) / best_cost * 100
    
    print(f"Best single model: {best_model} (cost: {best_cost:,.2f})")
    print(f"Selector cost: {selector_cost:,.2f}")
    print(f"Improvement: {improvement:.1f}%")
    print()
    print('âœ… Evaluation complete and validated')


if __name__ == '__main__':
    main()

