{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comprehensive Time Series EDA - VN2 Competition\n",
        "\n",
        "**Parallel Processing on Apple M2 Max (12 cores, 64GB RAM)**\n",
        "\n",
        "This notebook performs deep exploratory data analysis on ~3 years of weekly sales data:\n",
        "- Summary statistics & dispersion coefficients\n",
        "- Stationarity & heteroskedasticity tests (ADF, KPSS, ARCH)\n",
        "- ACF/PACF analysis\n",
        "- Lag analysis & lead-lag relationships\n",
        "- Frequency domain analysis (periodograms)\n",
        "- Master SLURP construction with metadata preservation\n",
        "- Bootstrap analysis leveraging relationship preservation\n",
        "\n",
        "**Optimization**: All analyses parallelized across 11 workers, memory-optimized data structures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Environment & Imports\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '12'\n",
        "os.environ['MKL_NUM_THREADS'] = '12'\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '12'\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '../src')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from joblib import Parallel, delayed\n",
        "from multiprocessing import cpu_count\n",
        "import warnings\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "\n",
        "N_JOBS = 11  # Leave 1 core for system\n",
        "print(f\"ðŸš€ Using {N_JOBS} parallel workers on {cpu_count()} cores\")\n",
        "print(f\"ðŸ“Š NumPy using {np.show_config()}\")\n",
        "\n",
        "@contextmanager\n",
        "def timer(name):\n",
        "    start = time.perf_counter()\n",
        "    yield\n",
        "    elapsed = time.perf_counter() - start\n",
        "    print(f\"â±ï¸  {name}: {elapsed:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load & Prepare Data\n",
        "\n",
        "Load all raw files and reshape to long format for analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with timer(\"Data loading\"):\n",
        "    # Load raw files\n",
        "    sales = pd.read_csv('../data/raw/Week 0 - 2024-04-08 - Sales.csv')\n",
        "    stock = pd.read_csv('../data/raw/Week 0 - In Stock.csv')\n",
        "    master = pd.read_csv('../data/raw/Week 0 - Master.csv')\n",
        "    \n",
        "    # Melt to long format\n",
        "    sales_long = sales.melt(\n",
        "        id_vars=['Store', 'Product'], \n",
        "        var_name='week', \n",
        "        value_name='sales'\n",
        "    )\n",
        "    sales_long['week_date'] = pd.to_datetime(sales_long['week'])\n",
        "    sales_long['year'] = sales_long['week_date'].dt.year\n",
        "    sales_long['retail_week'] = sales_long['week_date'].dt.isocalendar().week\n",
        "    sales_long['month'] = sales_long['week_date'].dt.month\n",
        "    \n",
        "    stock_long = stock.melt(\n",
        "        id_vars=['Store', 'Product'], \n",
        "        var_name='week', \n",
        "        value_name='in_stock'\n",
        "    )\n",
        "    \n",
        "    # Merge\n",
        "    df = sales_long.merge(\n",
        "        stock_long, \n",
        "        on=['Store', 'Product', 'week']\n",
        "    ).merge(\n",
        "        master,\n",
        "        on=['Store', 'Product']\n",
        "    ).sort_values(['Store', 'Product', 'week']).reset_index(drop=True)\n",
        "\n",
        "# Memory optimization\n",
        "df_optimized = df.copy()\n",
        "df_optimized['Store'] = df_optimized['Store'].astype('int16')\n",
        "df_optimized['Product'] = df_optimized['Product'].astype('int16')\n",
        "df_optimized['sales'] = df_optimized['sales'].astype('float32')\n",
        "df_optimized['year'] = df_optimized['year'].astype('int16')\n",
        "df_optimized['retail_week'] = df_optimized['retail_week'].astype('int8')\n",
        "df_optimized['month'] = df_optimized['month'].astype('int8')\n",
        "df_optimized['in_stock'] = df_optimized['in_stock'].astype('bool')\n",
        "\n",
        "for col in ['ProductGroup', 'Division', 'Department', 'DepartmentGroup', 'StoreFormat', 'Format']:\n",
        "    if col in df_optimized.columns:\n",
        "        df_optimized[col] = df_optimized[col].astype('category')\n",
        "\n",
        "mem_before = df.memory_usage(deep=True).sum() / 1024**2\n",
        "mem_after = df_optimized.memory_usage(deep=True).sum() / 1024**2\n",
        "\n",
        "df = df_optimized  # Use optimized version\n",
        "\n",
        "print(f\"\\nðŸ“¦ Data shape: {df.shape}\")\n",
        "print(f\"ðŸ“… Date range: {df['week_date'].min()} to {df['week_date'].max()}\")\n",
        "print(f\"ðŸª Stores: {df['Store'].nunique()}, Products: {df['Product'].nunique()}\")\n",
        "print(f\"ðŸ’¾ Memory: {mem_before:.1f} MB â†’ {mem_after:.1f} MB ({(1-mem_after/mem_before)*100:.1f}% reduction)\")\n",
        "print(f\"\\nâœ… All data loaded and in memory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Master SLURP Construction\n",
        "\n",
        "Create one grand SLURP where each row = one SKU-week observation with all metadata preserved for bootstrap analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from vn2.uncertainty import SLURP\n",
        "\n",
        "with timer(\"SLURP construction\"):\n",
        "    # Create scenario ID\n",
        "    df['scenario_id'] = df.index\n",
        "    \n",
        "    # Build SIP dictionary with all variables\n",
        "    sip_dict = {\n",
        "        'sales': df['sales'].values,\n",
        "        'store': df['Store'].values,\n",
        "        'product': df['Product'].values,\n",
        "        'retail_week': df['retail_week'].values,\n",
        "        'year': df['year'].values,\n",
        "        'month': df['month'].values,\n",
        "        'in_stock': df['in_stock'].astype(int).values,  # 1/0 for True/False\n",
        "        'product_group': df['ProductGroup'].cat.codes.values,\n",
        "        'department': df['Department'].cat.codes.values,\n",
        "        'store_format': df['StoreFormat'].cat.codes.values,\n",
        "    }\n",
        "    \n",
        "    # Metadata\n",
        "    meta_df = pd.DataFrame({\n",
        "        'field_type': ['continuous', 'discrete', 'discrete', 'discrete', 'discrete', 'discrete',\n",
        "                       'binary', 'discrete', 'discrete', 'discrete'],\n",
        "        'description': ['Weekly sales', 'Store ID', 'Product ID', 'ISO week', 'Year', 'Month',\n",
        "                        'Stock availability', 'Product group', 'Department', 'Store format']\n",
        "    }, index=list(sip_dict.keys())).T\n",
        "    \n",
        "    # Create master SLURP\n",
        "    master_slurp = SLURP.from_dict(\n",
        "        sip_dict, \n",
        "        meta=meta_df,\n",
        "        provenance=\"VN2 Historical Demand 2021-2024 with Metadata\"\n",
        "    )\n",
        "    \n",
        "    # Save\n",
        "    master_slurp.to_xml('../data/processed/master_demand_slurp.xml', csvr=4, average=True, median=True)\n",
        "    df.to_parquet('../data/processed/demand_long.parquet')\n",
        "\n",
        "print(f\"âœ… SLURP created: {master_slurp.n_scenarios:,} scenarios\")\n",
        "print(f\"   Variables: {list(master_slurp.names)}\")\n",
        "print(f\"   Saved to: ../data/processed/master_demand_slurp.xml\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Summary Statistics & Dispersion (Parallel)\n",
        "\n",
        "Compute comprehensive statistics for all 599 SKUs in parallel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "def compute_sku_summary(sku_data):\n",
        "    \"\"\"Compute all summary stats for one SKU\"\"\"\n",
        "    store, product = sku_data.name\n",
        "    sales = sku_data['sales'].values\n",
        "    non_zero = sales[sales > 0]\n",
        "    \n",
        "    return {\n",
        "        'Store': store,\n",
        "        'Product': product,\n",
        "        'n_weeks': len(sales),\n",
        "        'mean': sales.mean(),\n",
        "        'std': sales.std(),\n",
        "        'min': sales.min(),\n",
        "        'q25': np.percentile(sales, 25),\n",
        "        'median': np.median(sales),\n",
        "        'q75': np.percentile(sales, 75),\n",
        "        'max': sales.max(),\n",
        "        'cv': sales.std() / (sales.mean() + 1e-9),\n",
        "        'qcd': (np.percentile(sales, 75) - np.percentile(sales, 25)) / \n",
        "               (np.percentile(sales, 75) + np.percentile(sales, 25) + 1e-9),\n",
        "        'mad': np.median(np.abs(sales - np.median(sales))),\n",
        "        'mad_mean_ratio': np.median(np.abs(sales - np.median(sales))) / (sales.mean() + 1e-9),\n",
        "        'iqr': np.percentile(sales, 75) - np.percentile(sales, 25),\n",
        "        'range': sales.max() - sales.min(),\n",
        "        'skewness': skew(sales),\n",
        "        'kurtosis': kurtosis(sales),\n",
        "        'pct_zeros': (sales == 0).mean(),\n",
        "        'pct_stockout': (~sku_data['in_stock']).mean(),\n",
        "        'adi': len(sales) / len(non_zero) if len(non_zero) > 0 else np.inf,\n",
        "    }\n",
        "\n",
        "with timer(\"Summary statistics (parallel)\"):\n",
        "    sku_groups = list(df.groupby(['Store', 'Product']))\n",
        "    summary_list = Parallel(n_jobs=N_JOBS, verbose=1)(\n",
        "        delayed(compute_sku_summary)(grp) for _, grp in sku_groups\n",
        "    )\n",
        "    summary = pd.DataFrame(summary_list)\n",
        "    summary.to_parquet('../data/processed/summary_statistics.parquet')\n",
        "\n",
        "print(f\"\\nâœ… Summary stats computed for {len(summary)} SKUs\")\n",
        "print(f\"\\nOverall statistics:\")\n",
        "print(summary[['mean', 'std', 'cv', 'skewness', 'pct_zeros', 'pct_stockout']].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Visualize Dispersion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# CV vs Mean (Heteroskedasticity)\n",
        "summary.plot.scatter(x='mean', y='cv', alpha=0.3, ax=axes[0], s=10)\n",
        "axes[0].set_title('CV vs Mean (Heteroskedasticity)')\n",
        "axes[0].set_xscale('log')\n",
        "axes[0].set_yscale('log')\n",
        "axes[0].set_xlabel('Mean Sales')\n",
        "axes[0].set_ylabel('Coefficient of Variation')\n",
        "\n",
        "# MAD/Mean vs Mean\n",
        "summary.plot.scatter(x='mean', y='mad_mean_ratio', alpha=0.3, ax=axes[1], s=10, c='orange')\n",
        "axes[1].set_title('MAD/Mean Ratio vs Mean')\n",
        "axes[1].set_xscale('log')\n",
        "axes[1].set_xlabel('Mean Sales')\n",
        "axes[1].set_ylabel('MAD/Mean Ratio')\n",
        "\n",
        "# Skewness distribution\n",
        "summary['skewness'].hist(bins=50, ax=axes[2], edgecolor='black', alpha=0.7)\n",
        "axes[2].set_title('Skewness Distribution')\n",
        "axes[2].set_xlabel('Skewness')\n",
        "axes[2].axvline(0, color='red', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Kurtosis distribution\n",
        "summary['kurtosis'].hist(bins=50, ax=axes[3], edgecolor='black', alpha=0.7, color='green')\n",
        "axes[3].set_title('Excess Kurtosis Distribution')\n",
        "axes[3].set_xlabel('Excess Kurtosis')\n",
        "axes[3].axvline(0, color='red', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Intermittency: Zeros vs ADI\n",
        "summary.plot.scatter(x='pct_zeros', y='adi', alpha=0.3, ax=axes[4], s=10, c='purple')\n",
        "axes[4].set_title('Intermittency: % Zeros vs ADI')\n",
        "axes[4].set_xlabel('% Zeros')\n",
        "axes[4].set_ylabel('Average Demand Interval')\n",
        "axes[4].set_yscale('log')\n",
        "\n",
        "# Stockout vs CV\n",
        "summary.plot.scatter(x='pct_stockout', y='cv', alpha=0.3, ax=axes[5], s=10, c='red')\n",
        "axes[5].set_title('Stockout Rate vs CV')\n",
        "axes[5].set_xlabel('% Stockout Weeks')\n",
        "axes[5].set_ylabel('Coefficient of Variation')\n",
        "axes[5].set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/dispersion_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"ðŸ’¾ Saved: ../reports/dispersion_analysis.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Stationarity & Heteroskedasticity Tests (Parallel)\n",
        "\n",
        "ADF (Augmented Dickey-Fuller), KPSS, and ARCH-LM tests for all SKUs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.stats.diagnostic import het_arch\n",
        "from scipy import signal\n",
        "\n",
        "def test_stationarity_parallel(sku_data):\n",
        "    \"\"\"Test stationarity & heteroskedasticity for one SKU\"\"\"\n",
        "    store, product = sku_data.name\n",
        "    series = sku_data['sales'].dropna()\n",
        "    \n",
        "    results = {'Store': store, 'Product': product}\n",
        "    \n",
        "    # ADF test (null: non-stationary)\n",
        "    try:\n",
        "        adf = adfuller(series, autolag='AIC', maxlag=12)\n",
        "        results['adf_stat'] = adf[0]\n",
        "        results['adf_pval'] = adf[1]\n",
        "        results['adf_stationary'] = adf[1] < 0.05\n",
        "    except:\n",
        "        results['adf_stat'] = results['adf_pval'] = results['adf_stationary'] = np.nan\n",
        "    \n",
        "    # KPSS test (null: stationary)\n",
        "    try:\n",
        "        kpss_res = kpss(series, regression='c', nlags='auto')\n",
        "        results['kpss_stat'] = kpss_res[0]\n",
        "        results['kpss_pval'] = kpss_res[1]\n",
        "        results['kpss_stationary'] = kpss_res[1] > 0.05\n",
        "    except:\n",
        "        results['kpss_stat'] = results['kpss_pval'] = results['kpss_stationary'] = np.nan\n",
        "    \n",
        "    # ARCH test (conditional heteroskedasticity)\n",
        "    try:\n",
        "        detrended = signal.detrend(series)\n",
        "        if len(detrended) > 10:\n",
        "            arch = het_arch(detrended, nlags=min(5, len(detrended)//4))\n",
        "            results['arch_stat'] = arch[0]\n",
        "            results['arch_pval'] = arch[1]\n",
        "            results['heteroskedastic'] = arch[1] < 0.05\n",
        "        else:\n",
        "            results['arch_stat'] = results['arch_pval'] = results['heteroskedastic'] = np.nan\n",
        "    except:\n",
        "        results['arch_stat'] = results['arch_pval'] = results['heteroskedastic'] = np.nan\n",
        "    \n",
        "    return results\n",
        "\n",
        "with timer(\"Stationarity tests (parallel)\"):\n",
        "    stationarity_list = Parallel(n_jobs=N_JOBS, verbose=1)(\n",
        "        delayed(test_stationarity_parallel)(grp) for _, grp in sku_groups\n",
        "    )\n",
        "    stationarity_tests = pd.DataFrame(stationarity_list)\n",
        "    stationarity_tests.to_parquet('../data/processed/stationarity_tests.parquet')\n",
        "\n",
        "print(f\"\\nâœ… Stationarity tests complete for {len(stationarity_tests)} SKUs\")\n",
        "print(f\"\\nTest results:\")\n",
        "print(f\"  ADF stationary: {stationarity_tests['adf_stationary'].sum()} / {len(stationarity_tests)} ({stationarity_tests['adf_stationary'].mean()*100:.1f}%)\")\n",
        "print(f\"  KPSS stationary: {stationarity_tests['kpss_stationary'].sum()} / {len(stationarity_tests)} ({stationarity_tests['kpss_stationary'].mean()*100:.1f}%)\")\n",
        "print(f\"  Heteroskedastic (ARCH): {stationarity_tests['heteroskedastic'].sum()} / {len(stationarity_tests)} ({stationarity_tests['heteroskedastic'].mean()*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ACF/PACF Analysis (Parallel)\n",
        "\n",
        "Autocorrelation and partial autocorrelation for lag identification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "\n",
        "def compute_acf_pacf_parallel(sku_data, nlags=52):\n",
        "    \"\"\"Compute ACF/PACF for one SKU\"\"\"\n",
        "    store, product = sku_data.name\n",
        "    series = sku_data['sales'].dropna()\n",
        "    \n",
        "    if len(series) < nlags + 1:\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        acf_vals = acf(series, nlags=nlags, fft=True)  # FFT faster\n",
        "        pacf_vals = pacf(series, nlags=nlags, method='ywm')\n",
        "        \n",
        "        # Find significant lags\n",
        "        n = len(series)\n",
        "        threshold = 1.96 / np.sqrt(n)\n",
        "        sig_lags_acf = np.where(np.abs(acf_vals[1:]) > threshold)[0] + 1\n",
        "        sig_lags_pacf = np.where(np.abs(pacf_vals[1:]) > threshold)[0] + 1\n",
        "        \n",
        "        return {\n",
        "            'Store': store,\n",
        "            'Product': product,\n",
        "            'acf': acf_vals,\n",
        "            'pacf': pacf_vals,\n",
        "            'sig_lags_acf': sig_lags_acf,\n",
        "            'sig_lags_pacf': sig_lags_pacf,\n",
        "            'n_sig_acf': len(sig_lags_acf),\n",
        "            'n_sig_pacf': len(sig_lags_pacf)\n",
        "        }\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "with timer(\"ACF/PACF computation (parallel)\"):\n",
        "    acf_list = Parallel(n_jobs=N_JOBS, verbose=1)(\n",
        "        delayed(compute_acf_pacf_parallel)(grp, nlags=52) for _, grp in sku_groups\n",
        "    )\n",
        "    acf_results = {(r['Store'], r['Product']): r for r in acf_list if r is not None}\n",
        "\n",
        "# Aggregate ACF/PACF across SKUs\n",
        "all_acf_data = []\n",
        "for k, v in acf_results.items():\n",
        "    for lag in range(len(v['acf'])):\n",
        "        all_acf_data.append({\n",
        "            'lag': lag,\n",
        "            'acf': v['acf'][lag],\n",
        "            'pacf': v['pacf'][lag] if lag < len(v['pacf']) else np.nan\n",
        "        })\n",
        "\n",
        "all_acf_df = pd.DataFrame(all_acf_data)\n",
        "median_acf = all_acf_df.groupby('lag')[['acf', 'pacf']].median()\n",
        "\n",
        "print(f\"âœ… ACF/PACF computed for {len(acf_results)} SKUs\")\n",
        "\n",
        "# Plot aggregate ACF/PACF\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].stem(median_acf.index, median_acf['acf'], basefmt=' ')\n",
        "axes[0].axhline(0, color='black', linewidth=0.8)\n",
        "axes[0].axhline(1.96/np.sqrt(157), color='red', linestyle='--', label='95% CI')\n",
        "axes[0].axhline(-1.96/np.sqrt(157), color='red', linestyle='--')\n",
        "axes[0].set_title('Median ACF Across All SKUs')\n",
        "axes[0].set_xlabel('Lag (weeks)')\n",
        "axes[0].set_ylabel('ACF')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].stem(median_acf.index, median_acf['pacf'], basefmt=' ')\n",
        "axes[1].axhline(0, color='black', linewidth=0.8)\n",
        "axes[1].axhline(1.96/np.sqrt(157), color='red', linestyle='--', label='95% CI')\n",
        "axes[1].axhline(-1.96/np.sqrt(157), color='red', linestyle='--')\n",
        "axes[1].set_title('Median PACF Across All SKUs')\n",
        "axes[1].set_xlabel('Lag (weeks)')\n",
        "axes[1].set_ylabel('PACF')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/acf_pacf_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"ðŸ’¾ Saved: ../reports/acf_pacf_analysis.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Frequency Domain Analysis (Vectorized)\n",
        "\n",
        "Periodogram analysis to identify dominant periodicities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.fft import rfft, rfftfreq\n",
        "\n",
        "with timer(\"Frequency analysis (vectorized)\"):\n",
        "    # Reshape to matrix [n_skus, n_weeks]\n",
        "    sales_pivot = df.pivot(index=['Store', 'Product'], columns='week', values='sales').fillna(0)\n",
        "    sales_matrix = sales_pivot.values\n",
        "    \n",
        "    # Detrend\n",
        "    sales_centered = sales_matrix - sales_matrix.mean(axis=1, keepdims=True)\n",
        "    \n",
        "    # FFT on all SKUs at once\n",
        "    fft_result = rfft(sales_centered, axis=1)\n",
        "    power = np.abs(fft_result) ** 2\n",
        "    \n",
        "    # Frequencies\n",
        "    n_weeks = sales_matrix.shape[1]\n",
        "    freqs = rfftfreq(n_weeks, d=1.0)\n",
        "    \n",
        "    # Find top 5 dominant frequencies per SKU\n",
        "    top_k = 5\n",
        "    dominant_freq_idx = np.argsort(power, axis=1)[:, -top_k:][:, ::-1]\n",
        "\n",
        "# Extract dominant periods\n",
        "dominant_periods_list = []\n",
        "for i, (store, product) in enumerate(sales_pivot.index):\n",
        "    idx = dominant_freq_idx[i]\n",
        "    dominant_freqs = freqs[idx]\n",
        "    periods = 1.0 / (dominant_freqs + 1e-9)\n",
        "    periods = periods[(periods > 1) & (periods < 100)]\n",
        "    \n",
        "    dominant_periods_list.append({\n",
        "        'Store': store,\n",
        "        'Product': product,\n",
        "        'n_dominant_periods': len(periods),\n",
        "        'top_period': periods[0] if len(periods) > 0 else np.nan\n",
        "    })\n",
        "\n",
        "dominant_periods_df = pd.DataFrame(dominant_periods_list)\n",
        "\n",
        "print(f\"âœ… Frequency analysis complete\")\n",
        "print(f\"\\nDominant period distribution:\")\n",
        "print(dominant_periods_df['top_period'].describe())\n",
        "\n",
        "# Plot sample periodograms\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "sample_idx = [0, 100, 200, 300]\n",
        "for ax_i, data_i in enumerate(sample_idx):\n",
        "    if data_i < len(sales_pivot):\n",
        "        store, product = sales_pivot.index[data_i]\n",
        "        psd = power[data_i]\n",
        "        \n",
        "        ax = axes[ax_i]\n",
        "        ax.semilogy(freqs[1:], psd[1:])  # Skip DC component\n",
        "        ax.set_title(f'Store {store}, Product {product}')\n",
        "        ax.set_xlabel('Frequency (cycles/week)')\n",
        "        ax.set_ylabel('Power Spectral Density')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Mark dominant periods\n",
        "        idx = dominant_freq_idx[data_i]\n",
        "        for j in idx[:3]:\n",
        "            if freqs[j] > 0:\n",
        "                period = 1.0 / freqs[j]\n",
        "                if 2 < period < 60:\n",
        "                    ax.axvline(freqs[j], color='red', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/frequency_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"ðŸ’¾ Saved: ../reports/frequency_analysis.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. SLURP-Based Bootstrap Analysis (Parallel)\n",
        "\n",
        "Bootstrap with relationship preservation across store, SKU, week, year, and stockout status.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bootstrap_chunk(slurp_data, chunk_id, n_bootstrap_per_chunk, seed_offset):\n",
        "    \"\"\"Bootstrap a chunk of iterations\"\"\"\n",
        "    np.random.seed(seed_offset + chunk_id)\n",
        "    \n",
        "    results = []\n",
        "    for b in range(n_bootstrap_per_chunk):\n",
        "        # Sample with replacement (preserves relationships)\n",
        "        idx = np.random.choice(len(slurp_data), size=len(slurp_data), replace=True)\n",
        "        sample = slurp_data.iloc[idx]\n",
        "        \n",
        "        # Compute aggregates\n",
        "        overall_mean = sample['sales'].mean()\n",
        "        overall_std = sample['sales'].std()\n",
        "        \n",
        "        # By stockout status\n",
        "        stockout_mask = sample['in_stock'] == 0\n",
        "        instock_mask = sample['in_stock'] == 1\n",
        "        \n",
        "        stockout_mean = sample[stockout_mask]['sales'].mean() if stockout_mask.any() else np.nan\n",
        "        instock_mean = sample[instock_mask]['sales'].mean() if instock_mask.any() else np.nan\n",
        "        \n",
        "        # By year\n",
        "        year_means = {}\n",
        "        for year in sample['year'].unique():\n",
        "            year_means[f'year_{year}'] = sample[sample['year'] == year]['sales'].mean()\n",
        "        \n",
        "        # By retail week (seasonal)\n",
        "        week_means = {}\n",
        "        for week in range(1, 53):\n",
        "            week_data = sample[sample['retail_week'] == week]\n",
        "            if len(week_data) > 0:\n",
        "                week_means[f'week_{week}'] = week_data['sales'].mean()\n",
        "        \n",
        "        result = {\n",
        "            'overall_mean': overall_mean,\n",
        "            'overall_std': overall_std,\n",
        "            'stockout_mean': stockout_mean,\n",
        "            'instock_mean': instock_mean,\n",
        "            **year_means,\n",
        "            **week_means\n",
        "        }\n",
        "        results.append(result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "with timer(\"Bootstrap analysis (parallel)\"):\n",
        "    n_bootstrap_total = 10000\n",
        "    n_chunks = N_JOBS\n",
        "    n_per_chunk = n_bootstrap_total // n_chunks\n",
        "    \n",
        "    bootstrap_results = Parallel(n_jobs=N_JOBS, verbose=1)(\n",
        "        delayed(bootstrap_chunk)(master_slurp.data, i, n_per_chunk, 42) \n",
        "        for i in range(n_chunks)\n",
        "    )\n",
        "    \n",
        "    # Flatten results\n",
        "    bootstrap_flat = [item for chunk in bootstrap_results for item in chunk]\n",
        "    bootstrap_df = pd.DataFrame(bootstrap_flat)\n",
        "    \n",
        "    # Save\n",
        "    import pickle\n",
        "    with open('../data/processed/bootstrap_distributions.pkl', 'wb') as f:\n",
        "        pickle.dump(bootstrap_df, f)\n",
        "\n",
        "print(f\"âœ… Bootstrap complete: {len(bootstrap_flat):,} iterations\")\n",
        "print(f\"\\nOverall demand statistics:\")\n",
        "print(f\"  Mean: {bootstrap_df['overall_mean'].mean():.2f} Â± {bootstrap_df['overall_mean'].std():.2f}\")\n",
        "print(f\"  Std: {bootstrap_df['overall_std'].mean():.2f} Â± {bootstrap_df['overall_std'].std():.2f}\")\n",
        "print(f\"\\nBy stock status:\")\n",
        "print(f\"  In-stock mean: {bootstrap_df['instock_mean'].mean():.2f} Â± {bootstrap_df['instock_mean'].std():.2f}\")\n",
        "print(f\"  Stockout mean: {bootstrap_df['stockout_mean'].mean():.2f} Â± {bootstrap_df['stockout_mean'].std():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Bootstrap Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Overall mean distribution\n",
        "axes[0, 0].hist(bootstrap_df['overall_mean'], bins=50, density=True, alpha=0.7, edgecolor='black')\n",
        "axes[0, 0].axvline(bootstrap_df['overall_mean'].mean(), color='red', linestyle='--', label='Mean')\n",
        "axes[0, 0].set_title('Bootstrap: Overall Mean Demand')\n",
        "axes[0, 0].set_xlabel('Mean Demand')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# By stockout status\n",
        "axes[0, 1].hist(bootstrap_df['instock_mean'].dropna(), bins=50, density=True, alpha=0.7, label='In Stock')\n",
        "axes[0, 1].hist(bootstrap_df['stockout_mean'].dropna(), bins=50, density=True, alpha=0.7, label='Stockout')\n",
        "axes[0, 1].set_title('Bootstrap: Mean Demand by Stock Status')\n",
        "axes[0, 1].set_xlabel('Mean Demand')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Seasonal pattern with confidence bands\n",
        "week_cols = [c for c in bootstrap_df.columns if c.startswith('week_')]\n",
        "weeks = sorted([int(c.split('_')[1]) for c in week_cols])\n",
        "weekly_means = [bootstrap_df[f'week_{w}'].mean() for w in weeks]\n",
        "weekly_lower = [bootstrap_df[f'week_{w}'].quantile(0.025) for w in weeks]\n",
        "weekly_upper = [bootstrap_df[f'week_{w}'].quantile(0.975) for w in weeks]\n",
        "\n",
        "axes[1, 0].plot(weeks, weekly_means, 'b-', label='Mean', linewidth=2)\n",
        "axes[1, 0].fill_between(weeks, weekly_lower, weekly_upper, alpha=0.3, label='95% CI')\n",
        "axes[1, 0].set_title('Bootstrap: Seasonal Pattern (Retail Week)')\n",
        "axes[1, 0].set_xlabel('Retail Week')\n",
        "axes[1, 0].set_ylabel('Mean Demand')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Year-over-year with CI\n",
        "year_cols = [c for c in bootstrap_df.columns if c.startswith('year_')]\n",
        "years = sorted([int(c.split('_')[1]) for c in year_cols])\n",
        "year_means = [bootstrap_df[f'year_{y}'].mean() for y in years]\n",
        "year_lower = [bootstrap_df[f'year_{y}'].quantile(0.025) for y in years]\n",
        "year_upper = [bootstrap_df[f'year_{y}'].quantile(0.975) for y in years]\n",
        "\n",
        "axes[1, 1].errorbar(years, year_means, \n",
        "                     yerr=[np.array(year_means)-np.array(year_lower), \n",
        "                           np.array(year_upper)-np.array(year_means)],\n",
        "                     fmt='o-', capsize=5, linewidth=2, markersize=8)\n",
        "axes[1, 1].set_title('Bootstrap: Year-over-Year Trend')\n",
        "axes[1, 1].set_xlabel('Year')\n",
        "axes[1, 1].set_ylabel('Mean Demand')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/bootstrap_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"ðŸ’¾ Saved: ../reports/bootstrap_analysis.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary & Key Findings\n",
        "\n",
        "All analyses complete! Artifacts saved to `data/processed/` and `reports/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"COMPREHENSIVE TIME SERIES EDA - SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nðŸ“Š Dataset:\")\n",
        "print(f\"  â€¢ {df.shape[0]:,} observations (SKU-weeks)\")\n",
        "print(f\"  â€¢ {df['Store'].nunique()} stores Ã— {df['Product'].nunique()} products\")\n",
        "print(f\"  â€¢ {len(df['week'].unique())} weeks ({df['week_date'].min()} to {df['week_date'].max()})\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Summary Statistics:\")\n",
        "print(f\"  â€¢ Mean CV: {summary['cv'].mean():.2f} (high dispersion)\")\n",
        "print(f\"  â€¢ Skewness (median): {summary['skewness'].median():.2f}\")\n",
        "print(f\"  â€¢ Intermittent SKUs (ADI>1.32): {(summary['adi'] > 1.32).sum()} / {len(summary)} ({(summary['adi'] > 1.32).mean()*100:.1f}%)\")\n",
        "print(f\"  â€¢ Stockout rate (median): {summary['pct_stockout'].median()*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nðŸ”¬ Stationarity Tests:\")\n",
        "print(f\"  â€¢ ADF stationary: {stationarity_tests['adf_stationary'].sum()} / {len(stationarity_tests)} ({stationarity_tests['adf_stationary'].mean()*100:.1f}%)\")\n",
        "print(f\"  â€¢ KPSS stationary: {stationarity_tests['kpss_stationary'].sum()} / {len(stationarity_tests)} ({stationarity_tests['kpss_stationary'].mean()*100:.1f}%)\")\n",
        "print(f\"  â€¢ Heteroskedastic: {stationarity_tests['heteroskedastic'].sum()} / {len(stationarity_tests)} ({stationarity_tests['heteroskedastic'].mean()*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nðŸ“¡ Frequency Analysis:\")\n",
        "print(f\"  â€¢ Median dominant period: {dominant_periods_df['top_period'].median():.1f} weeks\")\n",
        "print(f\"  â€¢ SKUs with clear seasonality: {dominant_periods_df['n_dominant_periods'].sum()}\")\n",
        "\n",
        "print(f\"\\nðŸ”„ Bootstrap Results (10,000 iterations):\")\n",
        "print(f\"  â€¢ Overall mean demand: {bootstrap_df['overall_mean'].mean():.2f} Â± {bootstrap_df['overall_mean'].std():.3f}\")\n",
        "print(f\"  â€¢ In-stock vs stockout difference: {bootstrap_df['instock_mean'].mean() - bootstrap_df['stockout_mean'].mean():.2f}\")\n",
        "\n",
        "print(f\"\\nðŸ’¾ Artifacts Saved:\")\n",
        "print(f\"  â€¢ ../data/processed/master_demand_slurp.xml\")\n",
        "print(f\"  â€¢ ../data/processed/demand_long.parquet\")\n",
        "print(f\"  â€¢ ../data/processed/summary_statistics.parquet\")\n",
        "print(f\"  â€¢ ../data/processed/stationarity_tests.parquet\")\n",
        "print(f\"  â€¢ ../data/processed/bootstrap_distributions.pkl\")\n",
        "print(f\"  â€¢ ../reports/dispersion_analysis.png\")\n",
        "print(f\"  â€¢ ../reports/acf_pacf_analysis.png\")\n",
        "print(f\"  â€¢ ../reports/frequency_analysis.png\")\n",
        "print(f\"  â€¢ ../reports/bootstrap_analysis.png\")\n",
        "\n",
        "print(f\"\\nâœ… Comprehensive EDA complete! Ready for forecasting and optimization.\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
